{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import optim\n",
    "from nltk.corpus import stopwords\n",
    "from models import utils, MLP\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yarakhater/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "pretrained_embeddings, fasttext_vocab = utils.get_fasttext()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_split = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('[\\W_]', ' ')\n",
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "train_df = pd.read_csv('./data/train_older_adult_annotations.csv',delimiter=',', encoding='latin-1')\n",
    "test_df = pd.read_csv('./data/test_annotations.csv', delimiter=',')\n",
    "df = pd.concat([test_df, train_df])\n",
    "\n",
    "age_anxiety_df = pd.read_csv('./data/age_anxiety_full_responses.csv', delimiter=',')\n",
    "age_experience_df = pd.read_csv('./data/age_experience_responses.csv', delimiter=',')\n",
    "demographics_df = pd.read_csv('./data/demographics_responses.csv', delimiter=',')\n",
    "anxiety_score_df = pd.read_csv('./data/respondent_anxiety_table.csv', delimiter=',')\n",
    "\n",
    "df1 = pd.merge(demographics_df, anxiety_score_df, on='respondent_id')\n",
    "merged_df = pd.merge(df, df1, on='respondent_id')\n",
    "\n",
    "sentiment_labels = ['Very negative','Somewhat negative','Neutral','Somewhat positive','Very positive']\n",
    "annotator_ids = merged_df['respondent_id'].unique().tolist()\n",
    "\n",
    "id2label = {index: row for (index, row) in enumerate(sentiment_labels)} \n",
    "label2id = {row: index for (index, row) in enumerate(sentiment_labels)}\n",
    "\n",
    "id2annotator = {index: row for (index, row) in enumerate(annotator_ids)}\n",
    "annotator2id = {row: index for (index, row) in enumerate(annotator_ids)}\n",
    "merged_df[\"annotation\"] = merged_df[\"annotation\"].map(label2id)\n",
    "merged_df[\"respondent_id\"] = merged_df[\"respondent_id\"].map(annotator2id)\n",
    "merged_df = utils.clean_text(merged_df, 'unit_text', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text_ids'] = merged_df['unit_text'].apply(lambda x: x.split())\n",
    "merged_df['text_ids'] = merged_df['text_ids'].apply(fasttext_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(annotation_split):\n",
    "    splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 0)\n",
    "    split = splitter.split(merged_df, groups=merged_df['unit_id'])\n",
    "    train_inds, test_inds = next(split)\n",
    "    train_df = merged_df.iloc[train_inds]\n",
    "    test_df = merged_df.iloc[test_inds]\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    test_df = test_df.sample(frac=1)\n",
    "else: #annotator split\n",
    "    #split into train and test with 70% of annotators in train and 30% in test\n",
    "    train_annotators, test_annotators = train_test_split(merged_df[\"respondent_id\"].unique(), train_size=0.7, random_state=0)\n",
    "    print(len(train_annotators), len(test_annotators))\n",
    "    train_df = merged_df[merged_df['respondent_id'].isin(train_annotators)]\n",
    "    test_df = merged_df[merged_df['respondent_id'].isin(test_annotators)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(0.9975, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 1.3726\n",
      "Epoch [2/20], Loss: 1.3666\n",
      "Epoch [3/20], Loss: 1.3754\n",
      "Epoch [4/20], Loss: 1.3832\n",
      "Epoch [5/20], Loss: 1.3862\n",
      "Epoch [6/20], Loss: 1.3894\n",
      "Epoch [7/20], Loss: 1.3899\n",
      "Epoch [8/20], Loss: 1.3938\n",
      "Epoch [9/20], Loss: 1.3953\n",
      "Epoch [10/20], Loss: 1.3944\n",
      "Epoch [11/20], Loss: 1.3943\n",
      "Epoch [12/20], Loss: 1.3934\n",
      "Epoch [13/20], Loss: 1.3907\n",
      "Epoch [14/20], Loss: 1.3842\n",
      "Epoch [15/20], Loss: 1.3801\n",
      "Epoch [16/20], Loss: 1.3749\n",
      "Epoch [17/20], Loss: 1.3697\n",
      "Epoch [18/20], Loss: 1.3676\n",
      "Epoch [19/20], Loss: 1.3687\n",
      "Epoch [20/20], Loss: 1.3640\n"
     ]
    }
   ],
   "source": [
    "classifier1 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['respondent_id'], train_df['annotation'], len(train_df[\"annotation\"].unique()), mode=\"text\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 41.20707596253902 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['respondent_id'], test_df['annotation'], classifier1, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0047, grad_fn=<MeanBackward0>) tensor(0.9955, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 1.3556\n",
      "Epoch [2/20], Loss: 1.3415\n",
      "Epoch [3/20], Loss: 1.3411\n",
      "Epoch [4/20], Loss: 1.3437\n",
      "Epoch [5/20], Loss: 1.3460\n",
      "Epoch [6/20], Loss: 1.3472\n",
      "Epoch [7/20], Loss: 1.3525\n",
      "Epoch [8/20], Loss: 1.3320\n",
      "Epoch [9/20], Loss: 1.3272\n",
      "Epoch [10/20], Loss: 1.3207\n",
      "Epoch [11/20], Loss: 1.3180\n",
      "Epoch [12/20], Loss: 1.3065\n",
      "Epoch [13/20], Loss: 1.3089\n",
      "Epoch [14/20], Loss: 1.2996\n",
      "Epoch [15/20], Loss: 1.3022\n",
      "Epoch [16/20], Loss: 1.3036\n",
      "Epoch [17/20], Loss: 1.2948\n",
      "Epoch [18/20], Loss: 1.2835\n",
      "Epoch [19/20], Loss: 1.2744\n",
      "Epoch [20/20], Loss: 1.2719\n"
     ]
    }
   ],
   "source": [
    "classifier_2 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['respondent_id'], train_df['annotation'], len(train_df[\"annotation\"].unique()), mode=\"text_annotators\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 48.88000438140095 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['respondent_id'], test_df['annotation'], classifier_2, mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0050, grad_fn=<MeanBackward0>) tensor(0.9945, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 1.2325\n",
      "Epoch [2/20], Loss: 1.2218\n",
      "Epoch [3/20], Loss: 1.2038\n",
      "Epoch [4/20], Loss: 1.1833\n",
      "Epoch [5/20], Loss: 1.1687\n",
      "Epoch [6/20], Loss: 1.1644\n",
      "Epoch [7/20], Loss: 1.1625\n",
      "Epoch [8/20], Loss: 1.1604\n",
      "Epoch [9/20], Loss: 1.1556\n",
      "Epoch [10/20], Loss: 1.1522\n",
      "Epoch [11/20], Loss: 1.1519\n",
      "Epoch [12/20], Loss: 1.1564\n",
      "Epoch [13/20], Loss: 1.1562\n",
      "Epoch [14/20], Loss: 1.1528\n",
      "Epoch [15/20], Loss: 1.1500\n",
      "Epoch [16/20], Loss: 1.1495\n",
      "Epoch [17/20], Loss: 1.1434\n",
      "Epoch [18/20], Loss: 1.1404\n",
      "Epoch [19/20], Loss: 1.1401\n",
      "Epoch [20/20], Loss: 1.1366\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['respondent_id'], train_df['annotation'], len(train_df[\"annotation\"].unique()), mode=\"text_groups\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 46.289501067966484 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['respondent_id'], test_df['annotation'], classifier_3, mode=\"text_groups\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAB HATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('[\\W_]', ' ')\n",
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "demographics_df = pd.read_csv('./data/ghc/AnnotatorIAT_and_Attitudes.csv',delimiter=',', encoding='latin-1')\n",
    "# train_df = pd.read_csv(\"data/ghc/ghc_train.tsv\", delimiter='\\t')\n",
    "# test_df = pd.read_csv(\"data/ghc/ghc_test.tsv\", delimiter='\\t')\n",
    "annotations_df = pd.read_csv(\"data/ghc/GabHateCorpus_annotations.tsv\", delimiter='\\t')\n",
    "annotations_df = annotations_df[['Annotator', 'Text', 'Hate',\"ID\"]]\n",
    "\n",
    "merged_df = utils.clean_text(annotations_df, 'Text', stop_words)\n",
    "merged_df['text_ids'] = merged_df['Text'].apply(lambda x: x.split())\n",
    "merged_df['text_ids'] = merged_df['text_ids'].apply(fasttext_vocab)\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 0)\n",
    "split = splitter.split(annotations_df, groups=annotations_df['ID'])\n",
    "train_inds, test_inds = next(split)\n",
    "train_df = annotations_df.iloc[train_inds]\n",
    "test_df = annotations_df.iloc[test_inds]\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0465, grad_fn=<MeanBackward0>) tensor(0.9536, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.3386\n",
      "Epoch [2/20], Loss: 0.3442\n",
      "Epoch [3/20], Loss: 0.3484\n",
      "Epoch [4/20], Loss: 0.3515\n",
      "Epoch [5/20], Loss: 0.3523\n",
      "Epoch [6/20], Loss: 0.3523\n",
      "Epoch [7/20], Loss: 0.3515\n",
      "Epoch [8/20], Loss: 0.3505\n",
      "Epoch [9/20], Loss: 0.3483\n",
      "Epoch [10/20], Loss: 0.3465\n",
      "Epoch [11/20], Loss: 0.3447\n",
      "Epoch [12/20], Loss: 0.3438\n",
      "Epoch [13/20], Loss: 0.3410\n",
      "Epoch [14/20], Loss: 0.3396\n",
      "Epoch [15/20], Loss: 0.3382\n",
      "Epoch [16/20], Loss: 0.3369\n",
      "Epoch [17/20], Loss: 0.3351\n",
      "Epoch [18/20], Loss: 0.3316\n",
      "Epoch [19/20], Loss: 0.3306\n",
      "Epoch [20/20], Loss: 0.3289\n"
     ]
    }
   ],
   "source": [
    "classifier_1 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['Annotator'],train_df['Hate'], len(train_df[\"Hate\"].unique()), mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 86.79093005380477 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['Annotator'], test_df['Hate'], classifier_1, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0098, grad_fn=<MeanBackward0>) tensor(1.0000, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.2561\n",
      "Epoch [2/20], Loss: 0.2558\n",
      "Epoch [3/20], Loss: 0.2547\n",
      "Epoch [4/20], Loss: 0.2526\n",
      "Epoch [5/20], Loss: 0.2592\n",
      "Epoch [6/20], Loss: 0.2610\n",
      "Epoch [7/20], Loss: 0.2645\n",
      "Epoch [8/20], Loss: 0.2684\n",
      "Epoch [9/20], Loss: 0.2709\n",
      "Epoch [10/20], Loss: 0.2745\n",
      "Epoch [11/20], Loss: 0.2716\n",
      "Epoch [12/20], Loss: 0.2719\n",
      "Epoch [13/20], Loss: 0.2723\n",
      "Epoch [14/20], Loss: 0.2729\n",
      "Epoch [15/20], Loss: 0.2745\n",
      "Epoch [16/20], Loss: 0.2750\n",
      "Epoch [17/20], Loss: 0.2727\n",
      "Epoch [18/20], Loss: 0.2721\n",
      "Epoch [19/20], Loss: 0.2713\n",
      "Epoch [20/20], Loss: 0.2722\n"
     ]
    }
   ],
   "source": [
    "classifier_2 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['Annotator'],train_df['Hate'], len(train_df[\"Hate\"].unique()), mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 87.72482705611068 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['Annotator'], test_df['Hate'], classifier_2, mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0132, grad_fn=<MeanBackward0>) tensor(0.9628, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.2494\n",
      "Epoch [2/20], Loss: 0.2494\n",
      "Epoch [3/20], Loss: 0.2494\n",
      "Epoch [4/20], Loss: 0.2509\n",
      "Epoch [5/20], Loss: 0.2511\n",
      "Epoch [6/20], Loss: 0.2498\n",
      "Epoch [7/20], Loss: 0.2494\n",
      "Epoch [8/20], Loss: 0.2488\n",
      "Epoch [9/20], Loss: 0.2494\n",
      "Epoch [10/20], Loss: 0.2501\n",
      "Epoch [11/20], Loss: 0.2494\n",
      "Epoch [12/20], Loss: 0.2495\n",
      "Epoch [13/20], Loss: 0.2485\n",
      "Epoch [14/20], Loss: 0.2478\n",
      "Epoch [15/20], Loss: 0.2471\n",
      "Epoch [16/20], Loss: 0.2464\n",
      "Epoch [17/20], Loss: 0.2457\n",
      "Epoch [18/20], Loss: 0.2440\n",
      "Epoch [19/20], Loss: 0.2417\n",
      "Epoch [20/20], Loss: 0.2406\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['Annotator'],train_df['Hate'], len(train_df[\"Hate\"].unique()), mode=\"text_groups\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 86.72559569561875 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['Annotator'], test_df['Hate'], classifier_3, mode=\"text_groups\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('[\\W_]', ' ')\n",
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "annotations_df = pd.read_csv(\"data/Toxicity_content/toxic_content_annotation\", delimiter=',')\n",
    "text_df = pd.read_csv(\"data/Toxicity_content/toxic_content_sentences\", delimiter=',')\n",
    "annotators_df = pd.read_csv(\"data/Toxicity_content/toxic_content_workers\", delimiter=',')\n",
    "\n",
    "text_df = utils.clean_text(text_df, 'comment', stop_words)\n",
    "\n",
    "text_df['text_ids'] = text_df['comment'].apply(lambda x: x.split())\n",
    "text_df['text_ids'] = text_df['text_ids'].apply(fasttext_vocab)\n",
    "\n",
    "annotations_df[\"text_ids\"] = annotations_df[\"sentence_id\"].map(text_df.set_index(\"sentence_id\")[\"text_ids\"])\n",
    "\n",
    "\n",
    "annotator_ids = annotators_df['worker_id'].unique().tolist()\n",
    "id2annotator = {index: row for (index, row) in enumerate(annotator_ids)}\n",
    "annotator2id = {row: index for (index, row) in enumerate(annotator_ids)}\n",
    "annotations_df[\"worker_id\"] = annotations_df[\"worker_id\"].map(annotator2id)\n",
    "annotators_df[\"worker_id\"] = annotators_df[\"worker_id\"].map(annotator2id)\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 1)\n",
    "split = splitter.split(annotations_df, groups=annotations_df['sentence_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "train_df = annotations_df.iloc[train_inds]\n",
    "test_df = annotations_df.iloc[test_inds]\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    47370\n",
       "2    39828\n",
       "3    12449\n",
       "4     1303\n",
       "5       30\n",
       "Name: toxic_score, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show how many unique labels each sentence has\n",
    "x = annotations_df.groupby('sentence_id').agg({'toxic_score': lambda x: list(x)})\n",
    "x['toxic_score'].apply(lambda x: len(set(x))).value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002, grad_fn=<MeanBackward0>) tensor(1.0008, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 1.3204\n",
      "Epoch [2/20], Loss: 1.3210\n",
      "Epoch [3/20], Loss: 1.3192\n",
      "Epoch [4/20], Loss: 1.3142\n",
      "Epoch [5/20], Loss: 1.3087\n",
      "Epoch [6/20], Loss: 1.3051\n",
      "Epoch [7/20], Loss: 1.3014\n",
      "Epoch [8/20], Loss: 1.2993\n",
      "Epoch [9/20], Loss: 1.2967\n",
      "Epoch [10/20], Loss: 1.2952\n",
      "Epoch [11/20], Loss: 1.2945\n",
      "Epoch [12/20], Loss: 1.2923\n",
      "Epoch [13/20], Loss: 1.2884\n",
      "Epoch [14/20], Loss: 1.2882\n",
      "Epoch [15/20], Loss: 1.2853\n",
      "Epoch [16/20], Loss: 1.2846\n",
      "Epoch [17/20], Loss: 1.2823\n",
      "Epoch [18/20], Loss: 1.2816\n",
      "Epoch [19/20], Loss: 1.2798\n",
      "Epoch [20/20], Loss: 1.2797\n"
     ]
    }
   ],
   "source": [
    "classifier_1 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['worker_id'], train_df['toxic_score'], len(train_df[\"toxic_score\"].unique()), mode=\"text\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 50.221939496748654 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['worker_id'], test_df['toxic_score'], classifier_1, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.9485e-05, grad_fn=<MeanBackward0>) tensor(1.0016, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 1.1792\n",
      "Epoch [2/20], Loss: 1.1004\n",
      "Epoch [3/20], Loss: 1.0954\n",
      "Epoch [4/20], Loss: 1.0756\n",
      "Epoch [5/20], Loss: 1.0536\n",
      "Epoch [6/20], Loss: 1.0278\n",
      "Epoch [7/20], Loss: 0.9926\n",
      "Epoch [8/20], Loss: 0.9761\n",
      "Epoch [9/20], Loss: 0.9617\n",
      "Epoch [10/20], Loss: 0.9552\n",
      "Epoch [11/20], Loss: 0.9399\n",
      "Epoch [12/20], Loss: 0.9162\n",
      "Epoch [13/20], Loss: 0.8946\n",
      "Epoch [14/20], Loss: 0.8656\n",
      "Epoch [15/20], Loss: 0.8720\n",
      "Epoch [16/20], Loss: 0.8654\n",
      "Epoch [17/20], Loss: 0.8738\n",
      "Epoch [18/20], Loss: 0.8584\n",
      "Epoch [19/20], Loss: 0.8430\n",
      "Epoch [20/20], Loss: 0.8084\n"
     ]
    }
   ],
   "source": [
    "classifier_2 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['worker_id'], train_df['toxic_score'], len(train_df[\"toxic_score\"].unique()), mode=\"text_annotators\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 47.414475544246535 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['worker_id'], test_df['toxic_score'], classifier_2, mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(0.9987, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.4921\n",
      "Epoch [2/20], Loss: 0.4584\n",
      "Epoch [3/20], Loss: 0.4343\n",
      "Epoch [4/20], Loss: 0.4192\n",
      "Epoch [5/20], Loss: 0.4109\n",
      "Epoch [6/20], Loss: 0.4019\n",
      "Epoch [7/20], Loss: 0.4001\n",
      "Epoch [8/20], Loss: 0.3995\n",
      "Epoch [9/20], Loss: 0.3985\n",
      "Epoch [10/20], Loss: 0.3999\n",
      "Epoch [11/20], Loss: 0.3991\n",
      "Epoch [12/20], Loss: 0.4007\n",
      "Epoch [13/20], Loss: 0.4035\n",
      "Epoch [14/20], Loss: 0.4025\n",
      "Epoch [15/20], Loss: 0.4015\n",
      "Epoch [16/20], Loss: 0.4029\n",
      "Epoch [17/20], Loss: 0.4016\n",
      "Epoch [18/20], Loss: 0.4058\n",
      "Epoch [19/20], Loss: 0.4036\n",
      "Epoch [20/20], Loss: 0.4000\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['worker_id'], train_df['toxic_score'], len(train_df[\"toxic_score\"].unique()), mode=\"text_groups\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 48.29092451229856 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['worker_id'], test_df['toxic_score'], classifier_3, mode=\"text_groups\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Emotions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('[\\W_]', ' ')\n",
      "/Users/yarakhater/Desktop/STAGE-POL/annotator_disagreement/models/utils.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[column_name] = df[column_name].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/goemotions/full_dataset/goemotions-full.csv', header=0, delimiter=';')\n",
    "df = df.drop(columns=['subreddit', 'link_id', 'parent_id', \"created_utc\"])\n",
    "\n",
    "\n",
    "with open('./data/goemotions/full_dataset/ekman_mapping.json', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "dictionary = json.loads(data)\n",
    "\n",
    "ekman_labels = list(dictionary.keys())\\\n",
    "\n",
    "#modify dataset to only contain ekman labels\n",
    "for key, value in dictionary.items():\n",
    "    dfs = []\n",
    "    for v in value:\n",
    "        dfs.append(df[v])\n",
    "        df = df.drop(columns=[v])\n",
    "    df_combined = pd.concat(dfs, axis=1)\n",
    "    df[key] = df_combined.iloc[:, 0:len(value)].apply(lambda x: int(x.any()), axis=1)\n",
    "\n",
    "row_sum = df[ekman_labels].sum(axis=1)\n",
    "has_multiple_classes = (row_sum > 1)\n",
    "\n",
    "# remove rows that have more than one class and those who are not annotated \n",
    "df = df[~has_multiple_classes & df[\"example_very_unclear\"] ==False]\n",
    "\n",
    "#add a \"label\" column that contains the index of the ekman label and not the name   \n",
    "df['label'] = df[ekman_labels].idxmax(axis=1)\n",
    "\n",
    "df = df[[\"id\",\"text\", \"label\", \"rater_id\"]]\n",
    "\n",
    "id2label = {index: row for (index, row) in enumerate(ekman_labels)} \n",
    "label2id = {row: index for (index, row) in enumerate(ekman_labels)}\n",
    "\n",
    "df[\"label\"] = df[\"label\"].map(label2id)\n",
    "# df[\"rater_id\"] = df[\"rater_id\"].astype(str)\n",
    "\n",
    "df = utils.clean_text(df, 'text', stop_words)\n",
    "\n",
    "df['text_ids'] = df['text'].apply(lambda x: x.split())\n",
    "df['text_ids'] = df['text_ids'].apply(fasttext_vocab)\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 7)\n",
    "split = splitter.split(df, groups=df['id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0213, grad_fn=<MeanBackward0>) tensor(1.0161, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.9764\n",
      "Epoch [2/20], Loss: 0.9305\n",
      "Epoch [3/20], Loss: 0.9039\n",
      "Epoch [4/20], Loss: 0.8889\n",
      "Epoch [5/20], Loss: 0.8839\n",
      "Epoch [6/20], Loss: 0.8926\n",
      "Epoch [7/20], Loss: 0.9049\n",
      "Epoch [8/20], Loss: 0.8957\n",
      "Epoch [9/20], Loss: 0.8863\n",
      "Epoch [10/20], Loss: 0.8857\n",
      "Epoch [11/20], Loss: 0.8899\n",
      "Epoch [12/20], Loss: 0.8907\n",
      "Epoch [13/20], Loss: 0.8864\n",
      "Epoch [14/20], Loss: 0.8634\n",
      "Epoch [15/20], Loss: 0.8402\n",
      "Epoch [16/20], Loss: 0.8331\n",
      "Epoch [17/20], Loss: 0.8351\n",
      "Epoch [18/20], Loss: 0.8324\n",
      "Epoch [19/20], Loss: 0.8183\n",
      "Epoch [20/20], Loss: 0.8085\n"
     ]
    }
   ],
   "source": [
    "classifier_1 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['rater_id'], train_df['label'], len(train_df[\"label\"].unique()), mode=\"text\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 54.294006349613575 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['rater_id'], test_df['label'], classifier_1, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0076, grad_fn=<MeanBackward0>) tensor(1.0044, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.9169\n",
      "Epoch [2/20], Loss: 0.8841\n",
      "Epoch [3/20], Loss: 0.8696\n",
      "Epoch [4/20], Loss: 0.8510\n",
      "Epoch [5/20], Loss: 0.8717\n",
      "Epoch [6/20], Loss: 0.8472\n",
      "Epoch [7/20], Loss: 0.8605\n",
      "Epoch [8/20], Loss: 0.8674\n",
      "Epoch [9/20], Loss: 0.8902\n",
      "Epoch [10/20], Loss: 0.8813\n",
      "Epoch [11/20], Loss: 0.8609\n",
      "Epoch [12/20], Loss: 0.8377\n",
      "Epoch [13/20], Loss: 0.8431\n",
      "Epoch [14/20], Loss: 0.8300\n",
      "Epoch [15/20], Loss: 0.8209\n",
      "Epoch [16/20], Loss: 0.8128\n",
      "Epoch [17/20], Loss: 0.8089\n",
      "Epoch [18/20], Loss: 0.7775\n",
      "Epoch [19/20], Loss: 0.7748\n",
      "Epoch [20/20], Loss: 0.7742\n"
     ]
    }
   ],
   "source": [
    "classifier_2 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['rater_id'], train_df['label'], len(train_df[\"label\"].unique()), mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 55.517429368566205 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['rater_id'], test_df['label'], classifier_2, mode=\"text_annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0149, grad_fn=<MeanBackward0>) tensor(1.0150, grad_fn=<VarBackward0>)\n",
      "Epoch [1/20], Loss: 0.2471\n",
      "Epoch [2/20], Loss: 0.2378\n",
      "Epoch [3/20], Loss: 0.2425\n",
      "Epoch [4/20], Loss: 0.2404\n",
      "Epoch [5/20], Loss: 0.2435\n",
      "Epoch [6/20], Loss: 0.2448\n",
      "Epoch [7/20], Loss: 0.2440\n",
      "Epoch [8/20], Loss: 0.2420\n",
      "Epoch [9/20], Loss: 0.2385\n",
      "Epoch [10/20], Loss: 0.2353\n",
      "Epoch [11/20], Loss: 0.2340\n",
      "Epoch [12/20], Loss: 0.2305\n",
      "Epoch [13/20], Loss: 0.2263\n",
      "Epoch [14/20], Loss: 0.2221\n",
      "Epoch [15/20], Loss: 0.2167\n",
      "Epoch [16/20], Loss: 0.2129\n",
      "Epoch [17/20], Loss: 0.2079\n",
      "Epoch [18/20], Loss: 0.2047\n",
      "Epoch [19/20], Loss: 0.2018\n",
      "Epoch [20/20], Loss: 0.1975\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = MLP.train(pretrained_embeddings, train_df['text_ids'], train_df['rater_id'], train_df['label'], len(train_df[\"label\"].unique()), mode=\"text_groups\", nb_groups = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 54.194593207837606 %\n"
     ]
    }
   ],
   "source": [
    "predicted = MLP.test(test_df['text_ids'], test_df['rater_id'], test_df['label'], classifier_3, mode=\"text_groups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
