{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19360efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import optim\n",
    "from nltk.corpus import stopwords\n",
    "from models import bert\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial\n",
    "import os\n",
    "os.environ[\"RAY_FUNCTION_SIZE_ERROR_THRESHOLD\"] = \"1000\"  # Set a higher threshold value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcabc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_new_agr.csv',delimiter=',', encoding='latin-1')\n",
    "test_df = pd.read_csv('test_new_agr.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960626a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['disagreement']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44859832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>is_age_related</th>\n",
       "      <th>unique_annotations</th>\n",
       "      <th>disagreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>10000484</td>\n",
       "      <td>830</td>\n",
       "      <td>Getting old isn't easy and the little things s...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>28</td>\n",
       "      <td>10001142</td>\n",
       "      <td>1390</td>\n",
       "      <td>People in the younger of these two groups are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>54</td>\n",
       "      <td>10002181</td>\n",
       "      <td>1390</td>\n",
       "      <td>People in the younger of these two groups are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>61</td>\n",
       "      <td>10002442</td>\n",
       "      <td>830</td>\n",
       "      <td>Getting old isn't easy and the little things s...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>86</td>\n",
       "      <td>10003461</td>\n",
       "      <td>1390</td>\n",
       "      <td>People in the younger of these two groups are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>1299</td>\n",
       "      <td>10051984</td>\n",
       "      <td>11915</td>\n",
       "      <td>That patronizing look of pity makes me want to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1307</td>\n",
       "      <td>10052275</td>\n",
       "      <td>11512</td>\n",
       "      <td>I have several friends who vehemently deny the...</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>1310</td>\n",
       "      <td>10052395</td>\n",
       "      <td>11512</td>\n",
       "      <td>I have several friends who vehemently deny the...</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>1316</td>\n",
       "      <td>10052660</td>\n",
       "      <td>11939</td>\n",
       "      <td>The younger adults' brain scans showed activit...</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1316</td>\n",
       "      <td>10052663</td>\n",
       "      <td>11952</td>\n",
       "      <td>But almost every other number from the bureau ...</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      annotator_id  annotation_id  unit_id  \\\n",
       "18              12       10000484      830   \n",
       "35              28       10001142     1390   \n",
       "55              54       10002181     1390   \n",
       "58              61       10002442      830   \n",
       "92              86       10003461     1390   \n",
       "...            ...            ...      ...   \n",
       "1242          1299       10051984    11915   \n",
       "1250          1307       10052275    11512   \n",
       "1254          1310       10052395    11512   \n",
       "1262          1316       10052660    11939   \n",
       "1263          1316       10052663    11952   \n",
       "\n",
       "                                                   text  annotation  \\\n",
       "18    Getting old isn't easy and the little things s...           1   \n",
       "35    People in the younger of these two groups are ...           1   \n",
       "55    People in the younger of these two groups are ...           1   \n",
       "58    Getting old isn't easy and the little things s...           1   \n",
       "92    People in the younger of these two groups are ...           1   \n",
       "...                                                 ...         ...   \n",
       "1242  That patronizing look of pity makes me want to...           1   \n",
       "1250  I have several friends who vehemently deny the...           2   \n",
       "1254  I have several friends who vehemently deny the...           2   \n",
       "1262  The younger adults' brain scans showed activit...           2   \n",
       "1263  But almost every other number from the bureau ...           1   \n",
       "\n",
       "     is_age_related  unique_annotations  disagreement  \n",
       "18              Yes                   1         False  \n",
       "35               No                   1         False  \n",
       "55              Yes                   1         False  \n",
       "58              Yes                   1         False  \n",
       "92              Yes                   1         False  \n",
       "...             ...                 ...           ...  \n",
       "1242            Yes                   1         False  \n",
       "1250            Yes                   1         False  \n",
       "1254            Yes                   1         False  \n",
       "1262             No                   1         False  \n",
       "1263             No                   1         False  \n",
       "\n",
       "[62 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577345a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_older_adult_annotations.csv',delimiter=',', encoding='latin-1')\n",
    "test_df = pd.read_csv('../data/test_annotations.csv', delimiter=',')\n",
    "df = pd.concat([test_df, train_df])\n",
    "\n",
    "age_anxiety_df = pd.read_csv('../data/age_anxiety_full_responses.csv', delimiter=',')\n",
    "age_experience_df = pd.read_csv('../data/age_experience_responses.csv', delimiter=',')\n",
    "demographics_df = pd.read_csv('../data/demographics_responses.csv', delimiter=',')\n",
    "anxiety_score_df = pd.read_csv('../data/respondent_anxiety_table.csv', delimiter=',')\n",
    "\n",
    "df1 = pd.merge(demographics_df, anxiety_score_df, on='respondent_id')\n",
    "merged_df = pd.merge(df, df1, on='respondent_id')\n",
    "\n",
    "sentiment_labels = ['Very negative','Somewhat negative','Neutral','Somewhat positive','Very positive']\n",
    "total_annotator_ids = merged_df['respondent_id'].unique().tolist()\n",
    "\n",
    "id2label = {index: row for (index, row) in enumerate(sentiment_labels)} \n",
    "label2id = {row: index for (index, row) in enumerate(sentiment_labels)}\n",
    "\n",
    "id2annotator = {index: row for (index, row) in enumerate(total_annotator_ids)}\n",
    "annotator2id = {row: index for (index, row) in enumerate(total_annotator_ids)}\n",
    "\n",
    "merged_df[\"annotation\"] = merged_df[\"annotation\"].map(label2id)\n",
    "merged_df[\"respondent_id\"] = merged_df[\"respondent_id\"].map(annotator2id)\n",
    "\n",
    "merged_df.rename(columns = {'respondent_id':'annotator_id', 'unit_text':'text'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a36670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1481, 1477, 1481)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 0)\n",
    "    split = splitter.split(merged_df, groups=merged_df['unit_id'])\n",
    "    train_inds, test_inds = next(split)\n",
    "    train_df = merged_df.iloc[train_inds]\n",
    "    test_val_df = merged_df.iloc[test_inds]\n",
    "    splitter = GroupShuffleSplit(test_size=0.5, n_splits=2, random_state = 0)\n",
    "    split = splitter.split(test_val_df, groups=test_val_df['unit_id'])\n",
    "    val_inds, test_inds = next(split)\n",
    "    val_df = test_val_df.iloc[val_inds]\n",
    "    test_df = test_val_df.iloc[test_inds]\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    test_df = test_df.sample(frac=1)    \n",
    "    val_df = val_df.sample(frac=1)\n",
    "    len(train_df[\"annotator_id\"].unique()), len(val_df[\"annotator_id\"].unique()), len(test_df[\"annotator_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26bc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = merged_df['annotation'].unique()\n",
    "#sort labels\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54bc0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68fd2160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels)).to(device)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of workers for data loaders\n",
    "batch_size = 8\n",
    "num_workers = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = bert.CustomDataset(train_df, tokenizer, labels)\n",
    "val_dataset = bert.CustomDataset(val_df, tokenizer, labels)\n",
    "test_dataset = bert.CustomDataset(test_df, tokenizer, labels)\n",
    "\n",
    "# Create training and testing data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb762dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-7, 1e-1),\n",
    "    \"weight_decay\": tune.choice([0.001, 0.01, 0.1, 0.2])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b012e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aaa2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"].sample(), weight_decay = config[\"weight_decay\"].sample())\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(\"epochh:\", epoch)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            annotator_ids = batch['annotator_id'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            if mode==\"groups\" :\n",
    "                w, log_p = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels =                   labels)\n",
    "                loss = torch.zeros(input_ids.size(0))\n",
    "                for i in range(input_ids.size(0)):\n",
    "                    loss[i] = - (w[i].log_softmax(dim=1) + log_p[i].reshape(-1, 1)).logsumexp(dim=0)[labels[i]]\n",
    "                # Backward pass and optimization\n",
    "                loss = torch.mean(loss)\n",
    "            elif mode==\"annotators\":\n",
    "                # Forward pass\n",
    "                outputs = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels =                     labels)\n",
    "                loss = outputs[0]\n",
    "            else:\n",
    "                outputs = model(input_ids =input_ids, attention_mask = attention_mask, labels = labels)\n",
    "                loss = outputs[0]\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_data_loader)\n",
    "        \n",
    "        total_val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in val_data_loader:\n",
    "            with torch.no_grad():\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                annotator_ids = batch['annotator_id'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                if mode==\"groups\" :\n",
    "                    w, log_p = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels = labels)\n",
    "                    loss = torch.zeros(input_ids.size(0))\n",
    "                    for i in range(input_ids.size(0)):\n",
    "                        loss[i] = - (w[i].log_softmax(dim=1) + log_p[i].reshape(-1, 1)).logsumexp(dim=0)[labels[i]]\n",
    "                    loss = torch.mean(loss)\n",
    "                    best_group = log_p.argmax(dim=1)\n",
    "                    w = w[range(len(w)), best_group]\n",
    "                    _, predicted = torch.max(w, 1)\n",
    "                elif mode==\"annotators\":\n",
    "                    # Forward pass\n",
    "                    outputs = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels = labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "                    # Get predicted labels\n",
    "                    _, predicted = torch.max(logits, dim=1)\n",
    "                else:\n",
    "                    outputs = model(input_ids =input_ids, attention_mask = attention_mask, labels = labels)\n",
    "                    print(outputs.shape, outputs)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "                    # Get predicted labels\n",
    "                    _, predicted = torch.max(logits, dim=1)\n",
    "                    \n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "    \n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "                \n",
    "        average_val_loss = total_val_loss / len(val_data_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": average_val_loss, \"accuracy\": accuracy},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10a9e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-04 11:38:09</td></tr>\n",
       "<tr><td>Running for: </td><td>00:16:21.51        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.9/62.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 0/32 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:P100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_33617_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">2.55557e-06</td><td style=\"text-align: right;\">         0.01 </td></tr>\n",
       "<tr><td>train_33617_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">2.22465e-06</td><td style=\"text-align: right;\">         0.001</td></tr>\n",
       "<tr><td>train_33617_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.00863107 </td><td style=\"text-align: right;\">         0.001</td></tr>\n",
       "<tr><td>train_33617_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.00743458 </td><td style=\"text-align: right;\">         0.1  </td></tr>\n",
       "<tr><td>train_33617_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1.51521e-06</td><td style=\"text-align: right;\">         0.1  </td></tr>\n",
       "<tr><td>train_33617_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0013465  </td><td style=\"text-align: right;\">         0.01 </td></tr>\n",
       "<tr><td>train_33617_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.016575   </td><td style=\"text-align: right;\">         0.001</td></tr>\n",
       "<tr><td>train_33617_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">8.59123e-05</td><td style=\"text-align: right;\">         0.2  </td></tr>\n",
       "<tr><td>train_33617_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1.59707e-05</td><td style=\"text-align: right;\">         0.2  </td></tr>\n",
       "<tr><td>train_33617_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.000925694</td><td style=\"text-align: right;\">         0.001</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=10,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "result = tune.run(\n",
    "    partial(train),\n",
    "    resources_per_trial={\"cpu\": 3, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "932d746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In neither tune session nor train session!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_data_loader, val_data_loader, mode)\u001b[0m\n\u001b[1;32m     41\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 44\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data_loader)\n\u001b[1;32m     48\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train(model, device, train_data_loader,val_data_loader, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'text.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db073d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.test(model, device, test_data_loader, mode=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f5bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 229\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# num_epochs = 10\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay = 0.01)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# schedule = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500,             num_training_steps=len(train_dataset)*num_epochs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# #     learning_rate=0.01,\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    212\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    213\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    214\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m#     learning_rate=1e-6\u001b[39;00m\n\u001b[1;32m    226\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTrainer_annotators_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;43;03m#     eval_dataset=test_dataset,\u001b[39;49;00m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:345\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_utils.py:93\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     91\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:183\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 183\u001b[0m         \u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the CUDA_LAUNCH_BLOCKING environment variable\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import bert\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, DistilBertForSequenceClassification, BertPreTrainedModel, BertModel, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train_new_agr.csv',delimiter=',', encoding='latin-1')\n",
    "test_df = pd.read_csv('test_new_agr.csv', delimiter=',')\n",
    "\n",
    "train_df = train_df[train_df['disagreement']==False]\n",
    "\n",
    "total_annotator_ids = train_df['annotator_id'].unique().tolist()\n",
    "\n",
    "\n",
    "train_labels = train_df['annotation'].unique()\n",
    "test_labels = test_df['annotation'].unique()\n",
    "labels = np.unique(np.concatenate((train_labels, test_labels), axis=0))\n",
    "#sort labels\n",
    "labels.sort()\n",
    "num_labels_glob=len(labels)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "configuration = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "configuration.num_labels = len(labels)\n",
    "configuration.num_annotators = len(total_annotator_ids)\n",
    "configuration.annotator_embedding_dim = 512\n",
    "configuration.hidden_size = 768 \n",
    "model = bert.BertForSequenceClassificationWithAnnotators(configuration)\n",
    "\n",
    "# configuration = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# configuration.num_labels = len(labels)\n",
    "# configuration.hidden_size = 768 \n",
    "\n",
    "# # model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels_glob)\n",
    "# model = bert.BertForSequenceClassificationText(configuration).to(device)\n",
    "# # model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels_glob)\n",
    "# # for param in model.bert.parameters():\n",
    "# #     param.requires_grad = False\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['text']\n",
    "        annotator_id = self.data.iloc[index]['annotator_id']\n",
    "        annotation = self.data.iloc[index]['annotation']\n",
    "        disagreement = self.data.iloc[index]['disagreement']\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        annotator_id = torch.tensor(annotator_id, dtype=torch.long)\n",
    "        annotation = torch.tensor(annotation, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'annotator_ids': annotator_id,\n",
    "            'label': annotation,\n",
    "            'disagreement': disagreement\n",
    "        }\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Define batch size and number of workers for data loaders\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "test_dataset = CustomDataset(test_df, tokenizer)\n",
    "\n",
    "# Create training and testing data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer_annotators_text(Trainer):\n",
    "    def compute_loss(self, model, inputs, device=torch.device(\"cuda\"), return_outputs=False):\n",
    "        \n",
    "        input_ids = inputs.get(\"input_ids\").to(device)\n",
    "        attention_mask = inputs.get(\"attention_mask\").to(device)\n",
    "        annotator_ids = inputs.get(\"annotator_ids\").to(device)\n",
    "        labels = inputs.get(\"labels\").to(device)\n",
    "        disagreement = inputs.get(\"disagreement\").to(device)\n",
    "        \n",
    "        outputs = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels = labels, freeze = True)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if return_outputs:\n",
    "#             return loss, outputs\n",
    "            return loss, {\"logits\":outputs[1], \"disagreement\":disagreement} \n",
    "        return loss\n",
    "\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#       'accuracy': acc,\n",
    "#     }\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions[0].argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    disagreement = pred.predictions[1]\n",
    "    labels_agreement = labels[~disagreement]\n",
    "    labels_disagreement = labels[disagreement]\n",
    "    predicted_agreement = preds[~disagreement]\n",
    "    predicted_disagreement = preds[disagreement] \n",
    "    agreement_acc = accuracy_score(labels_agreement, predicted_agreement)\n",
    "    disagreement_acc = accuracy_score(labels_disagreement, predicted_disagreement)\n",
    "    return {\n",
    "      'agreement_accuracy': agreement_acc,\n",
    "      'disagreement_accuracy':disagreement_acc,\n",
    "      'accuracy':acc\n",
    "    }\n",
    "\n",
    "\n",
    "# num_epochs = 10\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay = 0.01)\n",
    "# schedule = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500,             num_training_steps=len(train_dataset)*num_epochs)\n",
    "# optimizers = optimizer, schedule\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=num_epochs,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "# #     warmup_steps=500,\n",
    "# #     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=250,\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     remove_unused_columns=False,\n",
    "# #     optim= \"adamw_torch\",\n",
    "# #     learning_rate=0.01,\n",
    "# )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=250,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "#     learning_rate=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "trainer = CustomTrainer_annotators_text(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df24412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
