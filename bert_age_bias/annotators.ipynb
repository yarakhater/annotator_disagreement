{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19360efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from torch import optim\n",
    "from nltk.corpus import stopwords\n",
    "from models import bert\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel, BertConfig\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "577345a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_older_adult_annotations.csv',delimiter=',', encoding='latin-1')\n",
    "test_df = pd.read_csv('../data/test_annotations.csv', delimiter=',')\n",
    "df = pd.concat([test_df, train_df])\n",
    "\n",
    "age_anxiety_df = pd.read_csv('../data/age_anxiety_full_responses.csv', delimiter=',')\n",
    "age_experience_df = pd.read_csv('../data/age_experience_responses.csv', delimiter=',')\n",
    "demographics_df = pd.read_csv('../data/demographics_responses.csv', delimiter=',')\n",
    "anxiety_score_df = pd.read_csv('../data/respondent_anxiety_table.csv', delimiter=',')\n",
    "\n",
    "df1 = pd.merge(demographics_df, anxiety_score_df, on='respondent_id')\n",
    "merged_df = pd.merge(df, df1, on='respondent_id')\n",
    "\n",
    "sentiment_labels = ['Very negative','Somewhat negative','Neutral','Somewhat positive','Very positive']\n",
    "total_annotator_ids = merged_df['respondent_id'].unique().tolist()\n",
    "\n",
    "id2label = {index: row for (index, row) in enumerate(sentiment_labels)} \n",
    "label2id = {row: index for (index, row) in enumerate(sentiment_labels)}\n",
    "\n",
    "id2annotator = {index: row for (index, row) in enumerate(total_annotator_ids)}\n",
    "annotator2id = {row: index for (index, row) in enumerate(total_annotator_ids)}\n",
    "\n",
    "merged_df[\"annotation\"] = merged_df[\"annotation\"].map(label2id)\n",
    "merged_df[\"respondent_id\"] = merged_df[\"respondent_id\"].map(annotator2id)\n",
    "\n",
    "merged_df.rename(columns = {'respondent_id':'annotator_id', 'unit_text':'text'}, inplace = True)\n",
    "\n",
    "grouped = merged_df.groupby('unit_id')['annotation'].nunique().reset_index()\n",
    "grouped.columns = ['unit_id', 'unique_annotations']\n",
    "merged_df = merged_df.merge(grouped, on='unit_id')\n",
    "merged_df['disagreement'] = merged_df['unique_annotations'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a36670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    splitter = GroupShuffleSplit(test_size=0.3, n_splits=2, random_state = 0)\n",
    "    split = splitter.split(merged_df, groups=merged_df['unit_id'])\n",
    "    train_inds, test_inds = next(split)\n",
    "    train_df = merged_df.iloc[train_inds]\n",
    "    test_df = merged_df.iloc[test_inds]\n",
    "    train_df = train_df.sample(frac=1)\n",
    "    test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26bc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = merged_df['annotation'].unique()\n",
    "#sort labels\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "613e7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, labels):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['text']\n",
    "        annotator_id = self.data.iloc[index]['annotator_id']\n",
    "        annotation = self.data.iloc[index]['annotation']\n",
    "        disagreement = self.data.iloc[index]['disagreement']\n",
    "\n",
    "        # Tokenize the sentence \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        annotator_id = torch.tensor(annotator_id, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'annotator_id': annotator_id,\n",
    "            'label': annotation,\n",
    "            'disagreement': disagreement\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of workers for data loaders\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = CustomDataset(train_df, tokenizer, labels)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, labels)\n",
    "\n",
    "# Create training and testing data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "780f597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7fa6c3f49f30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59d0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e6befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "configuration = BertConfig()\n",
    "configuration.num_labels = len(labels)\n",
    "configuration.num_annotators = len(total_annotator_ids)\n",
    "configuration.annotator_embedding_dim = 100\n",
    "configuration.hidden_size = 768 \n",
    "model = bert.BertForSequenceClassificationWithAnnotators(configuration).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00a8995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, device, data_loader, mode):\n",
    "    model.eval()\n",
    "    correct_predictions_disagreement = 0\n",
    "    total_predictions_disagreement = 0\n",
    "    correct_predictions_agreement = 0\n",
    "    total_predictions_agreement = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            annotator_ids = batch['annotator_id'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            disagreement = batch['disagreement'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            if mode==\"groups\":\n",
    "                w, log_p = model(annotator_ids, input_ids, attention_mask=attention_mask)\n",
    "                best_group = log_p.argmax(dim=1)\n",
    "                w = w[range(len(w)), best_group]\n",
    "                _, predicted = torch.max(w, 1)\n",
    "\n",
    "                \n",
    "            elif mode == \"annotators\":\n",
    "                outputs = model(annotator_ids, input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[0]\n",
    "\n",
    "                # Get predicted labels\n",
    "                _, predicted = torch.max(logits, dim=1)\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[0]\n",
    "\n",
    "                # Get predicted labels\n",
    "                _, predicted = torch.max(logits, dim=1)\n",
    "            print(predicted,\"ppppppppp\",labels,\"jjjjjjj\", disagreement)\n",
    "                \n",
    "#             labels_agreement = [label for label, disagree in zip(labels, disagreement) if not disagree]\n",
    "#             labels_disagreement = [label for label, disagree in zip(labels, disagreement) if disagree]\n",
    "#             predicted_agreement = [predicted for predicted, disagree in zip(predicted, disagreement) if not disagree]\n",
    "#             predicted_disagreement = [predicted for predicted, disagree in zip(predicted, disagreement) if disagree]\n",
    "            labels_agreement = labels[~disagreement]\n",
    "            labels_disagreement = labels[disagreement]\n",
    "            predicted_agreement = predicted[~disagreement]\n",
    "            predicted_disagreement = labels[disagreement]\n",
    "            \n",
    "            print(predicted_disagreement, \"ppp\",labels_disagreement)\n",
    "                    \n",
    "\n",
    "            correct_predictions_disagreement += (predicted_disagreement == labels_disagreement).sum().item()\n",
    "            total_predictions_disagreement += labels_disagreement.size(0)\n",
    "            correct_predictions_agreement += (predicted_agreement == labels_agreement).sum().item()\n",
    "            total_predictions_agreement += labels_agreement.size(0)\n",
    "\n",
    "    accuracy_disagreement = correct_predictions_disagreement / total_predictions_disagreement\n",
    "    accuracy_agreement = correct_predictions_agreement / total_predictions_agreement\n",
    "    \n",
    "    return accuracy_disagreement, accuracy_agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a635d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') ppppppppp tensor([4, 1, 3, 3, 2, 1, 1, 0, 0, 2, 1, 1, 3, 2, 1, 2], device='cuda:0') jjjjjjj tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True], device='cuda:0')\n",
      "[tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0')] ppp [tensor(4, device='cuda:0'), tensor(1, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(3, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(2, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotators\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 46\u001b[0m, in \u001b[0;36mget_accuracy\u001b[0;34m(model, device, data_loader, mode)\u001b[0m\n\u001b[1;32m     41\u001b[0m predicted_disagreement \u001b[38;5;241m=\u001b[39m [predicted \u001b[38;5;28;01mfor\u001b[39;00m predicted, disagree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predicted, disagreement) \u001b[38;5;28;01mif\u001b[39;00m disagree]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_disagreement, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppp\u001b[39m\u001b[38;5;124m\"\u001b[39m,labels_disagreement)\n\u001b[0;32m---> 46\u001b[0m correct_predictions_disagreement \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted_disagreement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels_disagreement\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m total_predictions_disagreement \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels_disagreement\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m correct_predictions_agreement \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted_agreement \u001b[38;5;241m==\u001b[39m labels_agreement)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "get_accuracy(model, device, test_data_loader, mode=\"annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932d746b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotators\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/new_annotator_disagreement/Bert_Age_Bias/../models/bert.py:187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_data_loader, mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    186\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 187\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    190\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert.train(model, device, train_data_loader, mode=\"annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'annotators.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db073d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotators\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/new_annotator_disagreement/Bert_Age_Bias/../models/bert.py:229\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, device, test_data_loader, mode)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;66;03m# Get predicted labels\u001b[39;00m\n\u001b[1;32m    227\u001b[0m             _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m         correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m         total_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    232\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert.test(model, device, test_data_loader, mode=\"annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5bd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
