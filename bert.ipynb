{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "import matplotlib.pyplot as plt\n",
    "from models import utils\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertPreTrainedModel, BertModel, BertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv(\"data/Toxicity_content/toxic_content_annotation\", delimiter=',')\n",
    "text_df = pd.read_csv(\"data/Toxicity_content/toxic_content_sentences\", delimiter=',')\n",
    "annotators_df = pd.read_csv(\"data/Toxicity_content/toxic_content_workers\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>perspective_score</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just a matter of time before pick up on this s...</td>\n",
       "      <td>1135_1</td>\n",
       "      <td>0.280691</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this is QUINN you DUMBASS ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>1135_5</td>\n",
       "      <td>0.909117</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I like Maxi, long term for sure. Just wouldnâ€™t...</td>\n",
       "      <td>1135_6</td>\n",
       "      <td>0.051221</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>anna really out there embarrassing amber, iâ€™d ...</td>\n",
       "      <td>1135_7</td>\n",
       "      <td>0.785292</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mfw we need to purge the system</td>\n",
       "      <td>1135_8</td>\n",
       "      <td>0.366173</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100975</th>\n",
       "      <td>107615</td>\n",
       "      <td>Precisely. Drug testing does fuck-all to ensur...</td>\n",
       "      <td>7157_19</td>\n",
       "      <td>0.758788</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100976</th>\n",
       "      <td>107616</td>\n",
       "      <td>Adult women I know are generally smarter than ...</td>\n",
       "      <td>7157_20</td>\n",
       "      <td>0.363075</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100977</th>\n",
       "      <td>107617</td>\n",
       "      <td>This is as stupid as saying having a 3rd degre...</td>\n",
       "      <td>7157_21</td>\n",
       "      <td>0.766444</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100978</th>\n",
       "      <td>107618</td>\n",
       "      <td>\"Germany is the enemy of Judaism and must be p...</td>\n",
       "      <td>7157_22</td>\n",
       "      <td>0.607978</td>\n",
       "      <td>4chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100979</th>\n",
       "      <td>107619</td>\n",
       "      <td>\"Boo hoo, Tharizdun wants me to be his avatar!...</td>\n",
       "      <td>7157_23</td>\n",
       "      <td>0.344705</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100980 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id                                            comment  \\\n",
       "0                 0  Just a matter of time before pick up on this s...   \n",
       "1                 1                      this is QUINN you DUMBASS ðŸ˜­ðŸ˜­ðŸ˜­   \n",
       "2                 2  I like Maxi, long term for sure. Just wouldnâ€™t...   \n",
       "3                 3  anna really out there embarrassing amber, iâ€™d ...   \n",
       "4                 4                    mfw we need to purge the system   \n",
       "...             ...                                                ...   \n",
       "100975       107615  Precisely. Drug testing does fuck-all to ensur...   \n",
       "100976       107616  Adult women I know are generally smarter than ...   \n",
       "100977       107617  This is as stupid as saying having a 3rd degre...   \n",
       "100978       107618  \"Germany is the enemy of Judaism and must be p...   \n",
       "100979       107619  \"Boo hoo, Tharizdun wants me to be his avatar!...   \n",
       "\n",
       "       comment_id  perspective_score   source  \n",
       "0          1135_1           0.280691  twitter  \n",
       "1          1135_5           0.909117  twitter  \n",
       "2          1135_6           0.051221  twitter  \n",
       "3          1135_7           0.785292  twitter  \n",
       "4          1135_8           0.366173    4chan  \n",
       "...           ...                ...      ...  \n",
       "100975    7157_19           0.758788    4chan  \n",
       "100976    7157_20           0.363075    4chan  \n",
       "100977    7157_21           0.766444    4chan  \n",
       "100978    7157_22           0.607978    4chan  \n",
       "100979    7157_23           0.344705   reddit  \n",
       "\n",
       "[100980 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11604\n"
     ]
    }
   ],
   "source": [
    "annotations_df[\"comment\"] = annotations_df[\"sentence_id\"].map(text_df.set_index(\"sentence_id\")[\"comment\"])\n",
    "annotations_df[\"gender\"] = annotations_df[\"worker_id\"].map(annotators_df.set_index(\"worker_id\")[\"gender\"])\n",
    "\n",
    "x = annotations_df.groupby('sentence_id').agg({'toxic_score': lambda x: list(x)})\n",
    "#keep only sentences that have more than 1 unique annotation in annotations_df\n",
    "# x = x[x['toxic_score'].apply(lambda x: len(set(x))) > 1]\n",
    "# annotations_df = annotations_df[annotations_df['sentence_id'].isin(x.index)]\n",
    "\n",
    "annotators_df = annotators_df[annotators_df['worker_id'].isin(annotations_df['worker_id'])]\n",
    "print(len(annotators_df))\n",
    "\n",
    "total_annotator_ids = annotators_df['worker_id'].unique().tolist()\n",
    "id2annotator = {index: row for (index, row) in enumerate(total_annotator_ids)}\n",
    "annotator2id = {row: index for (index, row) in enumerate(total_annotator_ids)}\n",
    "annotations_df[\"worker_id\"] = annotations_df[\"worker_id\"].map(annotator2id)\n",
    "annotators_df[\"worker_id\"] = annotators_df[\"worker_id\"].map(annotator2id)\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=2, random_state = 2)\n",
    "split = splitter.split(annotations_df, groups=annotations_df['sentence_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "train_df = annotations_df.iloc[train_inds]\n",
    "test_df = annotations_df.iloc[test_inds]\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df['toxic_score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort labels\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config): \n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + config.annotator_embedding_dim, config.num_labels)\n",
    "        self.annotator_embeddings = nn.Embedding(config.num_annotators, config.annotator_embedding_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        annotator_ids=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        annotator_embeddings = self.annotator_embeddings(annotator_ids)\n",
    "\n",
    "        pooled_output = torch.cat((pooled_output, annotator_embeddings), dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig()\n",
    "configuration.num_labels = len(labels)\n",
    "configuration.num_annotators = len(total_annotator_ids)\n",
    "configuration.annotator_embedding_dim = 100\n",
    "configuration.hidden_size = 768 \n",
    "model = BertForSequenceClassification(configuration).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of workers for data loaders\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = utils.CustomDataset(train_df, tokenizer, labels)\n",
    "test_dataset = utils.CustomDataset(test_df, tokenizer, labels)\n",
    "\n",
    "# Create training and testing data loaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 Âµs, sys: 0 ns, total: 8 Âµs\n",
      "Wall time: 16.2 Âµs\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "%time\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        annotator_ids = batch['annotator_id'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(annotator_ids = annotator_ids, input_ids =input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        loss = outputs[0]\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_data_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
